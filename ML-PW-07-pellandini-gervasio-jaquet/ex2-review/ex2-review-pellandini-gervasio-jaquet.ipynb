{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "747b5d41",
   "metadata": {},
   "source": [
    "# Ex2 - Review questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e60f6082",
   "metadata": {},
   "source": [
    "### a) What are the two fundamental ideas a SVM are built on ? Summarize them with your own words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d929674",
   "metadata": {},
   "source": [
    "Les deux idées fondamentales pour SVM sont :\n",
    "\n",
    "1.\tMaximiser la marge :\n",
    "\n",
    "SVM cherche à trouver un hyperplan qui sépare au mieux les classes en maximisant la marge, c’est-à-dire la distance entre l’hyperplan et les points les plus proches de chaque classe (support vectors). En maximisant cette marge, le modèle devient plus robuste et généralise mieux les nouvelles données.\n",
    "\n",
    "On peut voir la marge comme un vecteur perpendiculaire à l’hyperplan de séparation, placé de manière à être à égale distance des points d’entraînement les plus proches de chaque classe.\n",
    "\n",
    "2.\tLe “kernel trick” :\n",
    "\n",
    "Pour les problèmes non linéaires, SVM projette les données dans un espace de dimension plus élevée. Cette transformation permet de rendre les classes séparables sans devoir effectuer manuellement de l’ingénierie de variables complexe. Ainsi, on peut utiliser des fonctions génériques qui donnent en moyenne de bonnes performances sans trop de réglages. Donc on évite le tuning manuel."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "266203b9",
   "metadata": {},
   "source": [
    "### b) With the hinge loss, training points can fall into three cases. Re-explain these cases with your own words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbbb3fe5",
   "metadata": {},
   "source": [
    "Cas 1 :\tles points correctement classés et en dehors de la marge.\n",
    "\n",
    "    Ces points sont bien classés et situés à l’extérieur de la marge. Ils ne génèrent aucune pénalité, car le modèle fait une bonne prédiction (loss = 0).\n",
    "\n",
    "Cas\t2 :\tles points correctement classés mais à l’intérieur de la marge.\n",
    "\n",
    "    Ces points sont du bon côté de la border de décision, mais trop proches de celle-ci. Ils se trouvent dans la zone de la marge et entraînent une petite pénalité comprise entre 0 et 1.\n",
    "\n",
    "Cas\t3.\tles points mal classés.\n",
    "\n",
    "    Ces points se trouvent du mauvais côté de l’hyperplan, donc dans la mauvaise classe. Ils subissent une forte pénalité, qui augmente avec la distance à la frontière de décision (la pénalité minimale est de 1)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18438801",
   "metadata": {},
   "source": [
    "### c) What are the two implementations of SVMs available in SciKit Learn ? Which one would you take if you have a system that needs to incorporate incremental learning ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "797101d2",
   "metadata": {},
   "source": [
    "1.\tSVC()\n",
    "\n",
    "SVC() utilise LibSVM, c'est un moteur d’optimisation numérique performant. SVC() résout le problème de minimisation sous contraintes, sans utiliser de descente de gradient.\n",
    "\n",
    "SVC() est particulièrement adaptée aux problèmes de classification standards, de petite à moyenne taille. Mais elle ne supporte pas l’apprentissage incrémental, car le modèle doit être entièrement réentraîné lorsqu’on ajoute de nouvelles données.\n",
    "\n",
    "2.\tSGDClassifier() \n",
    "\n",
    "SGDClassifier() utilise une descente de gradient stochastique pour optimiser un SVM linéaire. En configurant la fonction de perte sur \"hinge\" et la pénalité sur \"l2\", on obtient un comportement équivalent à un SVM linéaire.\n",
    "\n",
    "SGDClassifier() est mieux adapté aux grands ensembles de données et permet l’apprentissage incrémental, en gros la mise à jour du modèle se fait au fur et à mesure que de nouvelles données arrivent.\n",
    "\n",
    "    Donc pour la seconde question, il faudrait utiliser SGDClassifier() car plus adapté"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd848f8",
   "metadata": {},
   "source": [
    "### d) A SVM can classify between 2 classes. Cite and explain in your own words the 2 strategies we have to build a multi-class (with K classes) system with SVM ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8c2fefe",
   "metadata": {},
   "source": [
    "1. One vs one\n",
    "\n",
    "On entraîne un SVM pour chaque paire de classes, soit $\\frac{K(K-1)}{2}$ classificateurs au total.\n",
    "Chacun apprend à distinguer uniquement entre deux classes à la fois. Lorsqu’on prédit une nouvelle donnée, chaque modèle vote pour une classe, on sélectionne la classe en faisant un majority voting.\n",
    "C'est plus lourd en calculs, mais rend de bonnes performances car chaque modèle est plus spécialisé.\n",
    "\n",
    "2. One vs all\n",
    "\n",
    "Cela consiste à entraîner un SVM par classe. Pour chaque modèle une classe est considérée comme positive et toutes les autres comme négatives.\n",
    "Au total, on obtient donc K classificateurs. Lors de la prédiction, chaque SVM donne un score, et la classe avec le score le plus élevé est choisie comme résultat final.\n",
    "C'est efficace quand le nombre de classes n’est pas trop grand.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a6c6821",
   "metadata": {},
   "source": [
    "### e) Are the strategies of point d) equal in terms of cpu ? (elaborate your answer considering training and testing times)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca341ee1",
   "metadata": {},
   "source": [
    "1.\tOne-vs-One\n",
    "\n",
    "OnevsOne nécessite d’entraîner un classificateur SVM pour chaque paire de classes, soit $\\frac{K(K-1)}{2}$ modèles au total. Cela représente donc plus de modèles à entraîner.\n",
    "Mais chaque modèle n’utilise que les données des deux classes concernées, ça réduit la taille du jeu d’entraînement pour chaque SVM individuel.\n",
    "Le temps d’inférence (test) reste plus long, parce qu'il faut exécuter tous les classificateurs pour chaque nouvelle donnée avant de faire le vote final.\n",
    "\n",
    "2.\tOne-vs-All\n",
    "\n",
    "Cette méthode nécessite seulement K modèles, un par classe, donc moins de classificateurs à entraîner.\n",
    "Mais chaque modèle est entraîné sur l’ensemble du dataset ce qui peut rendre chaque entraînement plus coûteux.\n",
    "Le temps d’inférence est généralement plus court que pour OnevsOne, car il faut seulement comparer les K scores produits par les classificateurs.\n",
    "\n",
    "Le choix dépend donc du nombre de classes et de la taille du dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "458ede30",
   "metadata": {},
   "source": [
    "### f) Describe a machine learning task for which SVM would be a better choice than any of the algorithms previously studied. Explain why."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18048e25",
   "metadata": {},
   "source": [
    "On peut imaginer devoir classifier des images de chiffres manuscrits (mnist dataset). Ces images sont souvent non linéaires dans l’espace des caractéristiques.\n",
    "\n",
    "SVM est un meilleur choix car :\n",
    "\n",
    "Grâce au kernel trick, SVM peut projeter les données dans un espace de dimension supérieure, ce qui lui permet de séparer efficacement des classes dont les frontières sont non linéaires ce qui efficace pour des formes variées comme celles des chiffres écrit à la main.\n",
    "\n",
    "Aussi, le paramètre de régularisation C du SVM permet de contrôler le compromis entre marge et erreurs de classification, ce qui limite le risque d’overfitting, ce qui est important quand on traite les pixels d’une image par exemple.\n",
    "\n",
    "Grâce à la maximision de la marge entre les classes SVM tend à mieux généraliser sur de nouvelles données, contrairement à des méthodes comme KNN qui peuvent être sensibles au bruit.\n",
    "Comparé à KNN qui devient très lent lors de la phase de test, SVM une fois entraîné est plus rapide à prédire.\n",
    "\n",
    "Même si SVM est initialement binaire il peut être facilement étendu à plusieurs classes via les stratégies One-vs-One ou One-vs-All ce qui le rend  plus adapté à la classification multi-chiffres qui ont 10 classe.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
