{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d6c0f394",
   "metadata": {},
   "source": [
    "# Exercise 3 - Review questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5330e2ec",
   "metadata": {},
   "source": [
    "### a) Why do we have a gradient ascent in the case of logistic regression while we had a gradient descent with linear regression ? Can we convert the gradient ascent of logistic regression into a gradient descent ? If yes, how?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "108d69de",
   "metadata": {},
   "source": [
    "Dans la régression logistique, on maximise le \"likelihood\", nous utilisons donc l'ascension du gradient. Nous pourrions convertir ça en descente du gradient en minimisant plutôt le logarithme négatif du \"likelihood\". A l'invers, l'utilisation de la descente de gradient a été utilisée afin de minimiser la métrique MSE.\n",
    "\n",
    "Oui, au lieu d’utiliser le gradient ascent pour maximiser la log(likelihood), on peut simplement considérer l’opposé (-) comme une fonction de coût. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9000c86",
   "metadata": {},
   "source": [
    "### b) Assuming a logistic regression with a linear decision boundary taking as input samples in two dimensions (x1; x2), in which case do we get 0:5 as output of the classification system ? Express your answer with an equation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3adfff97",
   "metadata": {},
   "source": [
    "On obtient un résultat de 0,5 lorsque l'argument de la fonction sigmoïde est 0. $\\theta_{0} + \\theta_{1}x_1 + \\theta_{2}x_2 = 0$\n",
    "\n",
    "Ce point correspond au flip point, c'est la séparation où le modèle choisit entre les deux classes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df7d782c",
   "metadata": {},
   "source": [
    "### c) What is the computational trick to avoid numerical problems in the computation of J(theta) for the logistic regression ? In which situations (for what type of inputs) do we risk to observe such numerical problems ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9e405c9",
   "metadata": {},
   "source": [
    "Le trick pour éviter ces problème numériques lorsqu'on multiplie $J(\\theta)$ est d'ajouter un petit epsilon p.ex de $10^{-6}$ dans les log. Comme ça $h(x)$ ne va jamais atteidre exactement 0 ou 1, ça empêche d'avoir des valeurs indéfinies dans $\\log(h(x))$ et $\\log(1-h(x))$.\n",
    "On a ces problèmes de computation la plupard du temps avec des entrées \"extrêmes\" où $h(x)$ est très proche de 0 ou 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1657c670",
   "metadata": {},
   "source": [
    "    ### d) A logistic regression can classify between 2 classes. How can we build a multi-class (with K classes) system with logistic regression ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44c9e92c",
   "metadata": {},
   "source": [
    "On peut entrainer K modèles binaires distincts selon la stratégie « One vs Rest ».\n",
    "\n",
    "Chaque modèle apprend à distinguer une classe contre toutes les autres. Lors du test, on applique les K régressions logistiques et on choisit la classe qui retourne la probabilité la plus élevée. Comme expliqué Slide 31 du cours"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
