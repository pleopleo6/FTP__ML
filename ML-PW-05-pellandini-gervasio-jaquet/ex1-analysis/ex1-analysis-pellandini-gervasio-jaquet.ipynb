{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "33b31583",
   "metadata": {},
   "source": [
    "# Exercise 1 - Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e188fec",
   "metadata": {},
   "source": [
    "## Students : Leo Pellandini, Steven Jaquet et André Quintas Gervasio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51225f27",
   "metadata": {},
   "source": [
    "The bank UBS is offering the possibility to invest money in investment funds. A fund is composed of financial values such as stocks or bonds. For example, a fund composed mostly of stocks has more return potential but is more risky in case of stock market recession. There are thousands of funds available, see https://fundgate.ubs.com/. The probability to invest or not in a fund is conditioned by the profile of the fund and of the client. For example, a younger client with no child is potentially more interested into funds composed with stocks, showing higher risks but also higher potential returns. A family father will be more inclined to invest into low-risk funds. UBS want to build a system as illustrated on Figure~\\ref{fig:ubs_system}, taking as input a set of values characterizing the fund and the client profile.\n",
    "\n",
    "An investment fund can be characterized by the following elements: \n",
    "\n",
    "- The name of the fund.\n",
    "- The current value of 1 share in the fund, expressed in CHF.\n",
    "- The proportion of stock and bonds composing the fund (2 values in percentage).\n",
    "- A vector of float values with the 5 last yearly returns over years from 2015 to 2019 (5 values expressed in percentage).\n",
    "- A level of risk expressed with A, B, C, D, E with A representing the highest risk and E representing the lowest risk level.\n",
    "- A sectorial information such as technology, pharmaceutical, financial. There are 24 different sectors available in UBS funds.\n",
    "-  As the set of funds are worldwide, the emiting location is also available with the address of the managing entity of the fund, e.g. Market Street 1234, New York, USA.\n",
    "\n",
    "A client profile contains the following information: \n",
    "\n",
    "- First name and last name of the client.\n",
    "- The mother tongue of the client (mostly de, fr, it and en but other languages are present).\n",
    "- The age of the client.\n",
    "- The number of children of the client.\n",
    "- The current wealth of the client that could be used to buy funds, expressed in CHF (total of cash available in the different accounts, not yet invested in funds).\n",
    "- The postal code of the address of the client.\n",
    "- A level of acceptance to risk expressed with A, B, C, D, E with A representing the highest level of acceptance of risk and E representing the lowest acceptance of risk.\n",
    "\n",
    "Answer the following questions:\n",
    "\n",
    "1. For each available information in the fund and client profile, explain how you would prepare the data: encoding, normalization, outlier treatment, etc.\n",
    "2. How could you collect targets (output of the system) to train the system? How would you prepare the different sets?\n",
    "\n",
    "**Be as comprehensive as possible.**  Don't limit your explanation to the \"how\" but also the \"why\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb6b792",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f4c4b4",
   "metadata": {},
   "source": [
    "**For each available information in the fund and client profile, explain why and how you would prepare the data: encoding, normalization, outlier treatment, etc.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1527821",
   "metadata": {},
   "source": [
    "### About investment fund:\n",
    "\n",
    "- The **name of the fund** can be ignored for model training since it is not a quantitative piece of information useful for prediction. It can be used as a reference for the results.\n",
    "\n",
    "- The **numerical value of a share in CHF** requires normalization to prevent large values from dominating other features. We could use for example, min-max scaling between 0 and 1 or use Z-Norm. Also, check for outliers and exclude them if they are not realistic or not at all representative of the dataset.\n",
    "\n",
    "- Since the **percentages of stocks** and bonds are already on a standard scale, a min-max normalization can still be applied to keep them bounded between [0, 1]. A verification can be added to ensure that their sum equals 100%.\n",
    "\n",
    "- With **yearly returns** from 2015 to 2019, we have a time series, and the sliding-window method can be used. Methods such as Z-Norm can be applied to handle these values.\n",
    "\n",
    "- The **risk level (A to E)** is an ordinal variable, and an ordinal encoding can be used with these increasing levels, described as follow :\n",
    "\n",
    "<center>\n",
    "\n",
    "    \n",
    "|  Risk        | Encoding     |\n",
    "|---           |--:           |\n",
    "| A            | 5            |\n",
    "| B            | 4            |\n",
    "| C            | 3            |\n",
    "| D            | 2            |\n",
    "| E            | 1            |\n",
    "</center>\n",
    "\n",
    "(\"A\" representing the highest risk => 5)\n",
    "\n",
    "- We can do a 1-hot encoding for the **sectorial information** (because there is no relationship between sectors). This adds 24 columns to the dataset.\n",
    "\n",
    "- Regarding the **emitting locations**, using latitude and longitude coordinates are not particularly relevant in this context. Only the country of emission is kept (OHEncoding), as more detailed information such as street or city does not directly influence the fund’s performance or risk. A geographical bucketing by region or economic zone can be an interesting option to represent the location of the funds (also processed as OHE).\n",
    "\n",
    "\n",
    "### About client profile:\n",
    "\n",
    "- Once again, it is not necessary to take into account the client's **first and last name**, as this information is not useful for model training.\n",
    "\n",
    "- For the **native language**, there are four main languages in this context (DE, FR, IT, EN), so we can use a one-hot encoding and add an additional column that groups the other languages.\n",
    "\n",
    "- The **age** has no real outliers, and we can simply apply a min-max scaling between [0, 1], the min value being 0 and the maximum value the maximum value from the train set (or set up like 120 as max).\n",
    "\n",
    "- We can process the **number of children** in the same way as the age. It is important to ensure that there are no excessively high values, which should not be so frequent.\n",
    "\n",
    "- The **current wealth** is the same as the value of one share. There are some extreme values, either low or very high. We need to apply feature clipping and min-max rescaling for this.\n",
    "\n",
    "- **Postal codes** are categorical and present a discontinuity problem when treated numerically. Thus, we could use here one-hot encoding. However, since some postal codes are rare, grouping them into larger regions (ex Swiss cantons) helps reduce sparsity and may reveal more meaningful geographical patterns for our model.\n",
    "\n",
    "- As with the **risk level** of a fund, we can use an ordinal encoding for the risk acceptance levels with this specific encoding :\n",
    "\n",
    "<center>\n",
    "\n",
    "\n",
    "|  Risk        | Encoding     |\n",
    "|---           |--:           |\n",
    "| A            | 5            |\n",
    "| B            | 4            |\n",
    "| C            | 3            |\n",
    "| D            | 2            |\n",
    "| E            | 1            |\n",
    "</center>\n",
    "\n",
    "(\"A\" representing the highest level of acceptance of risk => 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c386d8fc",
   "metadata": {},
   "source": [
    "**How could you collect targets (output of the system) to train the system? How would you prepare the different sets?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ec9e135",
   "metadata": {},
   "source": [
    "### Collect the targets :\n",
    "\n",
    "To collect samples, we could use UBS’s historical data, mapping each client’s profile and the funds in which they have invested to their investment decision (go / no go).\n",
    " \n",
    "- y=1 : client invested\n",
    "- y=0 : client did not invest\n",
    "\n",
    "However, UBS probably only records the transactions that were actually made. To collect data about no-go decisions, the bank could, for example, prsent clients with a list of different funds and record both the ones they choose and the ones they did not invest in. Another option would be to get feedback from clients after fund recommendations to know which offers they decided not to invest in.\n",
    "\n",
    "Otherwise, we could proy the go / no go decisions by for example analyzing client behavior and trends on the UBS website [fundgate.ubs.com](https://fundgate.ubs.com/), identifying which types of funds are most frequently viewed by which types of clients, allowing to maybe proxy the decision.\n",
    "\n",
    "### Three independent data sets:\n",
    "\n",
    "For example, we use 60% of the collected data to train the model. Ensuring a good proportion of each class (funds chosen or not) to avoid an imbalance that could bias the predictions. If certain types of funds are chosen much more frequently than others (for example, low-risk funds that are often popular), techniques such as oversampling underrepresented funds / undersampling overrepresented ones should be applied to reduce bias (done to all the dataset).\n",
    "\n",
    "During training, we could apply k-fold cross-validation to create multiple training and validation splits. In each iteration, a different portion of the training data (here we choose 20%) would serve as the validation set. This approach helps achieve more robust hyperparameter tuning and reduces the risk of overfitting to a single validation split. To merge hyperparameters results, we should first analyse them and either merge them or maybe selecting manually.\n",
    "\n",
    "Once we have the best model, where the tuning of hyperparameters is done, we can use the reserved remaining 20% of the data for the final test. These data should not be used during the training phase, in order to evaluate the model’s generalization capability in the end.\n",
    "\n",
    "Finally, we may need to balance the datasets by randomly selecting samples and then verifying the distribution to ensure it is accurate. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
