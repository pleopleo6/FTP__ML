# K-Nearest Neighbors – Questions

## a)
The figure above illustrates the **K-nearest neighbors (K-NN)** algorithm using Euclidean
distance, represented by circles.  
Based on a **majority voting** and **shortest distance** (when tie), which class would the “Test” instance be assigned to with:

- \(K = 2\)
- \(K = 5\)
- \(K = 7\)
- \(K = 8\)

---

## b)
Explain in your own words the differences between **instance-based learning** and **model-based learning**.

---

## c)
Is **K-NN** an **instance-based learning** or a **model-based learning**?

---

## d)
When are **larger \(K\)** beneficial?

---

## e)
Why are **too large \(K\)** detrimental?

---

## f)
When used in **classification** mode, what can we do when the first categories have **equal number of votes** with a K-NN?

---

## g)
Are **K-NN algorithms** good candidates to build a **1’000 classes image classification system**?  
Explain your answer.

---

## h)
Is **K-NN** impacted by the *curse of dimensionality*? Explain your answer.

---

## i) *(Difficult)*
What is the **expected error rate** computed on the **training set** with \(K = 1\)?

---

## j) *(Difficult)*
What is the **expected error rate** computed on the **training set** with \(K = 2\) and a **shortest distance based tie resolution**?